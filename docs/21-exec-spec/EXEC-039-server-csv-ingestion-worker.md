---
# EXECUTION-SPEC Frontmatter
# Generated by build-pipeline (lead-architect scaffolding + expert consultation)

prd: PRD-039
prd_title: "Server-Authoritative CSV Ingestion Worker"
service: PlayerImportService
mvp_phase: 3  # Phase 3 (Onboarding & Intake)
generated: 2026-02-24
version: "1.2.0"

# Workstream Definitions
workstreams:
  WS1:
    name: Database Schema Extension
    description: "Extend import_batch_status enum, add worker lifecycle columns, create Storage bucket, patch RPCs (DA P0 fixes)"
    executor: backend-service-builder
    executor_type: skill
    depends_on: []
    outputs:
      - supabase/migrations/20260225145406_add_import_batch_worker_enums.sql
      - supabase/migrations/20260225145407_add_import_batch_worker_columns.sql
      - supabase/migrations/20260225145408_create_storage_bucket_imports.sql
      - supabase/migrations/20260225145409_alter_rpc_create_batch_overload.sql
      - supabase/migrations/20260225145410_fix_rpc_execute_reject_failed.sql
    gate: schema-validation
    estimated_complexity: medium

  WS2:
    name: Shared Header Normalization Module
    description: "Pure utility module for header/field normalization shared by client preview and worker"
    executor: backend-service-builder
    executor_type: skill
    depends_on: []
    outputs:
      - lib/csv/header-normalization.ts
      - lib/csv/index.ts
    gate: type-check
    estimated_complexity: low

  WS3:
    name: Worker Core Process
    description: "Standalone Node.js worker: poll → claim → stream → parse → normalize → validate → insert → heartbeat"
    executor: backend-service-builder
    executor_type: skill
    depends_on: [WS1, WS2]
    outputs:
      - workers/csv-ingestion/package.json
      - workers/csv-ingestion/tsconfig.json
      - workers/csv-ingestion/src/index.ts
      - workers/csv-ingestion/src/config.ts
      - workers/csv-ingestion/src/db.ts
      - workers/csv-ingestion/src/repo.ts
      - workers/csv-ingestion/src/claim.ts
      - workers/csv-ingestion/src/ingest.ts
      - workers/csv-ingestion/src/normalize.ts
      - workers/csv-ingestion/src/validate.ts
      - workers/csv-ingestion/src/storage.ts
      - workers/csv-ingestion/src/logger.ts
      - workers/csv-ingestion/src/health.ts
    gate: type-check
    estimated_complexity: high

  WS4:
    name: Upload Route Handler
    description: "POST /api/v1/player-import/batches/[id]/upload — multipart/form-data, Supabase Storage"
    executor: api-builder
    executor_type: skill
    depends_on: [WS1, WS5]
    outputs:
      - app/api/v1/player-import/batches/[id]/upload/route.ts
    gate: lint
    estimated_complexity: medium

  WS5:
    name: Service Layer Updates
    description: "Extend PlayerImportService DTOs, schemas, crud, http, mappers for worker lifecycle"
    executor: backend-service-builder
    executor_type: skill
    depends_on: [WS1]
    outputs:
      - services/player-import/dtos.ts
      - services/player-import/schemas.ts
      - services/player-import/crud.ts
      - services/player-import/http.ts
      - services/player-import/mappers.ts
      - services/player-import/index.ts
    gate: type-check
    estimated_complexity: medium

  WS6:
    name: UI Updates — Import Wizard
    description: "Update wizard step machine, new file-upload + worker-processing steps, polling with backoff"
    executor: frontend-design-pt-2
    executor_type: skill
    depends_on: [WS4, WS5]
    outputs:
      - hooks/player-import/use-import-wizard.ts
      - hooks/player-import/use-file-upload.ts
      - hooks/player-import/use-batch-polling.ts
      - components/player-import/import-wizard.tsx
      - components/player-import/step-file-upload.tsx
      - components/player-import/step-worker-processing.tsx
      - components/player-import/ingestion-report-card.tsx
    gate: build
    estimated_complexity: medium

  WS7:
    name: Unit Tests — Worker & Shared Modules
    description: "Header normalization, repo invariants, row validation, claim/reaper, row cap tests"
    executor: backend-service-builder
    executor_type: skill
    depends_on: [WS2, WS3]
    outputs:
      - lib/csv/__tests__/header-normalization.test.ts
      - workers/csv-ingestion/__tests__/repo.test.ts
      - workers/csv-ingestion/__tests__/normalize.test.ts
      - workers/csv-ingestion/__tests__/validate.test.ts
      - workers/csv-ingestion/__tests__/ingest.test.ts
    gate: test-pass
    estimated_complexity: medium

  WS8:
    name: Integration Tests
    description: "10k-row ingestion, cross-casino isolation, crash recovery, concurrent claim, denylist"
    executor: backend-service-builder
    executor_type: skill
    depends_on: [WS3, WS4, WS5]
    outputs:
      - workers/csv-ingestion/__tests__/ingest.int.test.ts
      - workers/csv-ingestion/__tests__/cross-casino.int.test.ts
      - workers/csv-ingestion/__tests__/crash-recovery.int.test.ts
      - workers/csv-ingestion/__tests__/concurrent-claim.int.test.ts
      - workers/csv-ingestion/__tests__/denylist.test.ts
      - services/player-import/__tests__/upload-route.int.test.ts
      - services/player-import/__tests__/execute-guard.int.test.ts
    gate: test-pass
    estimated_complexity: high

  WS9:
    name: E2E Tests — Server CSV Import
    description: "Happy path, error paths, progress display via Playwright"
    executor: e2e-testing
    executor_type: skill
    depends_on: [WS3, WS4, WS5, WS6]
    outputs:
      - e2e/workflows/csv-server-import.spec.ts
      - e2e/fixtures/sample-csvs/server-import-10-rows.csv
      - e2e/fixtures/sample-csvs/server-import-mixed.csv
    gate: test-pass
    estimated_complexity: medium

# Execution Phases (topologically sorted, parallelized where possible)
execution_phases:
  - name: "Phase 1 — Foundation (Schema & Shared Code)"
    parallel: [WS1, WS2]
    gates: [schema-validation, type-check]

  - name: "Phase 2 — Backend Core (Worker, Service Layer, Route Handler)"
    parallel: [WS3, WS4, WS5]
    gates: [type-check, lint]
    notes: "WS4 depends on WS1+WS5; execute after WS5 within this phase"

  - name: "Phase 3 — Frontend (Import Wizard Update)"
    parallel: [WS6]
    gates: [type-check, lint, build]

  - name: "Phase 4 — Testing (Unit, Integration, E2E)"
    parallel: [WS7, WS8, WS9]
    gates: [test-pass]

# Validation Gates
gates:
  schema-validation:
    command: "npm run db:types-local"
    success_criteria: "Exit code 0, types regenerated successfully"

  type-check:
    command: "npm run type-check"
    success_criteria: "Exit code 0"

  lint:
    command: "npm run lint -- --max-warnings=0"
    success_criteria: "Exit code 0, zero errors and zero warnings"

  build:
    command: "npm run build"
    success_criteria: "Exit code 0"

  test-pass:
    command: "npm test -- --passWithNoTests"
    success_criteria: "All tests pass"

# External Dependencies
external_dependencies:
  - prd: PRD-037
    service: PlayerImportService
    required_for: "Existing staging tables, RPCs, service layer, UI wizard (shipped)"

  - prd: ADR-036
    service: PlayerImportService
    required_for: "Lane 2 amendment (server-side parsing exit ramp)"

  - prd: ADR-037
    service: PlayerImportService
    required_for: "Worker runtime, security posture, status machine decisions (frozen)"

  - prd: ADR-024
    service: CasinoService
    required_for: "Authoritative context derivation (upload endpoint)"

  - prd: ADR-030
    service: CasinoService
    required_for: "Auth pipeline hardening (write-path enforcement)"

# Risks and Mitigations
risks:
  - risk: "Phase A coordination — batch status='created' default must deploy with upload endpoint"
    mitigation: "Phase 2 ships backend (worker + upload) before Phase 3 (UI). Old client flow unaffected."

  - risk: "Worker service_role credential exposure"
    mitigation: "Encrypted env vars only. All DB access through ImportBatchRepo (INV-W1–W7)."

  - risk: "Shared code packaging between Next.js app and worker"
    mitigation: "Monorepo workspace for MVP. Extract npm package if build friction becomes material."

  - risk: "Orphaned PII in Supabase Storage"
    mitigation: "Deferred to post-MVP retention policy (SEC note T7). GA gate documented."

  - risk: "Ingestion report lost after execute"
    mitigation: "Accepted for MVP (ADR-037 D4). UI shows ingestion report before execute trigger."
---

# EXECUTION-SPEC: PRD-039 — Server-Authoritative CSV Ingestion Worker

## Overview

PRD-039 delivers a standalone Node.js background worker that replaces client-side CSV parsing as the authoritative ingestion pathway. The worker claims uploaded CSV files from Supabase Storage, streams them through `csv-parse`, normalizes rows to the existing `ImportPlayerV1` contract, and bulk-inserts into `import_row`. This is the Lane 2 evolution documented in ADR-036/ADR-037, and unblocks the Loyalty & Tier Reconciliation pipeline.

## Scope

**In Scope:**
- File upload endpoint (multipart → Supabase Storage)
- Standalone Node.js worker (poll → claim → stream → parse → normalize → stage)
- `import_batch` schema extensions (worker lifecycle columns + new enum values)
- Crash recovery via reaper + heartbeat + idempotent inserts
- Row cap enforcement at 10,001 rows
- UI wizard update (file-upload + worker-processing steps replace client-side staging)
- Ingestion report display before execute

**Out of Scope:**
- `import_row` schema changes (zero)
- Loyalty field extraction or reconciliation
- Non-CSV formats, WebSocket progress, file retention, PII redaction
- Removal of `useStagingUpload` (deprecated, kept for rollback)

**Minimal RPC Patches (in scope per DA review):**
- `rpc_import_create_batch`: add optional `p_initial_status` parameter for server flow (`'created'`); existing 4-param signature unchanged
- `rpc_import_execute`: one-line safety fix — reject `failed` batches instead of silent return (PRD-039 §5.1 mandate)

## Architecture Context

- **SRM**: PlayerImportService owns `import_batch`, `import_row` (Onboarding Context)
- **ADR-036**: CSV Player Import Strategy — Lane 2 amendment (server-side parsing)
- **ADR-037**: Server-Authoritative CSV Ingestion Worker — Decisions D1–D4 (frozen)
- **ADR-024**: Authoritative context derivation (upload endpoint)
- **ADR-030**: Auth pipeline hardening (write-path enforcement)
- **SEC Note**: `docs/30-security/SEC-NOTE-SERVER-CSV-INGESTION.md`
- **Feature Boundary**: `docs/20-architecture/specs/server-csv-ingestion/FEATURE_BOUNDARY.md`

### Security Posture

Worker uses `service_role` (bypasses RLS) with SQL safety invariants:

| ID | Invariant | Enforcement |
|----|-----------|-------------|
| INV-W1 | Every `UPDATE import_batch` includes `WHERE id = $batch_id` | repo.ts |
| INV-W2 | Reaper includes `AND status = 'parsing' AND heartbeat_at < $threshold AND attempt_count < $max_attempts`; when `attempt_count >= max_attempts`, reaper transitions to `failed` instead of resetting to `uploaded` | repo.ts |
| INV-W3 | Every `INSERT INTO import_row` includes `batch_id` and `casino_id` from batch | repo.ts |
| INV-W4 | Worker never queries tables other than `import_batch` and `import_row` | repo.ts + CI denylist |
| INV-W5 | Worker never accepts `casino_id` from any source other than claimed batch row | repo.ts |
| INV-W6 | Claim CTE uses `WHERE status = 'uploaded'` only | repo.ts |
| INV-W7 | Worker may only set status to `parsing`, `staging`, or `failed` (reaper additionally sets `uploaded` for retry — see INV-W2) | repo.ts |

---

## Workstream Details

### WS1: Database Schema Extension

**Purpose**: Extend `import_batch` for worker lifecycle. Add Supabase Storage bucket.

**Deliverables**:
1. Migration `20260225145406_add_import_batch_worker_enums.sql`:
   - `ALTER TYPE import_batch_status ADD VALUE IF NOT EXISTS 'created'`
   - `ALTER TYPE import_batch_status ADD VALUE IF NOT EXISTS 'uploaded'`
   - `ALTER TYPE import_batch_status ADD VALUE IF NOT EXISTS 'parsing'`
   - Split from column migration — PostgreSQL cannot reference new enum values in same transaction (SQLSTATE 55P04)
2. Migration `20260225145407_add_import_batch_worker_columns.sql`:
   - Add columns: `storage_path text`, `original_file_name text`, `claimed_by text`, `claimed_at timestamptz`, `heartbeat_at timestamptz`, `attempt_count integer NOT NULL DEFAULT 0`, `last_error_at timestamptz`, `last_error_code text`
   - Partial index `idx_import_batch_status_uploaded` for worker claim query
   - Partial index `idx_import_batch_status_parsing_heartbeat` for reaper query
3. Migration `20260225145408_create_storage_bucket_imports.sql`:
   - Creates `imports` bucket (private, 10MB file size limit, no public access)
   - No `storage.objects` RLS policies created — all access is via `service_role` through API route and worker
4. Migration `20260225145409_alter_rpc_create_batch_overload.sql` **(DA P0-1 fix)**:
   - `CREATE OR REPLACE FUNCTION rpc_import_create_batch` — add optional `p_initial_status import_batch_status DEFAULT NULL` parameter
   - When `p_initial_status` is `'created'`, INSERT with `status = 'created'` instead of table default
   - When `p_initial_status` is NULL, use existing default `'staging'` (backward compat for old client flow)
   - Validate: only `NULL`, `'staging'`, or `'created'` accepted; others raise exception
   - Existing function signature (`text, text, text, jsonb`) kept via overload; new 5-param signature added
   - GRANT/REVOKE mirroring existing pattern
5. Migration `20260225145410_fix_rpc_execute_reject_failed.sql` **(DA P0-2 fix)**:
   - `CREATE OR REPLACE FUNCTION rpc_import_execute` — one-line fix
   - Change `IF v_batch.status IN ('completed', 'failed')` → `IF v_batch.status = 'completed'`
   - Now `failed` batches fall through to `IF v_batch.status != 'staging'` → raises `IMPORT_BATCH_NOT_STAGING`
   - This is the mandatory safety fix per PRD-039 §5.1

**Notes**:
- `import_batch.status` DEFAULT stays `'staging'` for backwards compat with the existing 4-param `rpc_import_create_batch` overload
- New server flow uses 5-param overload with `p_initial_status = 'created'`
- UNIQUE constraint `uq_import_row_batch_row` already exists (PRD-037). Migration `20260225145407` includes a guard assertion: `DO $$ BEGIN IF NOT EXISTS (SELECT 1 FROM pg_constraint WHERE conname = 'uq_import_row_batch_row') THEN RAISE EXCEPTION 'FATAL: UNIQUE(batch_id, row_number) constraint missing — ON CONFLICT DO NOTHING is load-bearing'; END IF; END $$;`
- Zero `import_row` schema changes
- `file_name` is the batch-creation-time label (set by user at batch creation). `original_file_name` is the actual uploaded file name (set by upload endpoint). They may differ.

**Acceptance Criteria**:
- [ ] `npm run db:types-local` succeeds after all migrations
- [ ] Enum values `created`, `uploaded`, `parsing` exist in `import_batch_status`
- [ ] All new columns present on `import_batch`
- [ ] `imports` Storage bucket exists with 10MB limit
- [ ] `rpc_import_create_batch(text, text, text, jsonb, import_batch_status)` overload exists and creates batch with `status = 'created'` when passed
- [ ] `rpc_import_create_batch(text, text, text, jsonb)` still creates batch with `status = 'staging'` (backward compat)
- [ ] `rpc_import_execute` on `failed` batch raises `IMPORT_BATCH_NOT_STAGING`
- [ ] `rpc_import_execute` on `completed` batch still returns silently (idempotent)

### WS2: Shared Header Normalization Module

**Purpose**: Pure utility module for header/field normalization, importable by both browser and Node.js.

**Deliverables**:
1. `lib/csv/header-normalization.ts` — exports:
   - `normalizeHeaders(rawHeaders: string[]): string[]` — trim, BOM strip, blank→`_col_N`, duplicate→`_N`, newline→space
   - `normalizeFieldValue(value: string | null | undefined): string | null` — trim, empty→null
   - `applyColumnMapping(rawRow, normalizedHeaders, columnMapping): Record<string, string | null>`
2. `lib/csv/index.ts` — barrel export

**Acceptance Criteria**:
- [ ] Module importable in both browser and Node.js (no Node-only APIs)
- [ ] Header normalization rules match SCAFFOLD-SCI §Header Normalization Rules
- [ ] `npm run type-check` passes

### WS3: Worker Core Process

**Purpose**: Standalone Node.js worker deployed on dedicated PaaS (Railway/Fly.io).

**Deliverables**:
1. `workers/csv-ingestion/package.json` — dependencies: `csv-parse`, `pg`, `@supabase/supabase-js`
2. `workers/csv-ingestion/tsconfig.json` — paths to root `lib/` and `services/`
3. `workers/csv-ingestion/src/index.ts` — entry point: init pool, poll loop, graceful shutdown, health server
4. `workers/csv-ingestion/src/config.ts` — env-based config with validation
5. `workers/csv-ingestion/src/db.ts` — `pg` pool (max 5 connections, `statement_timeout`)
6. `workers/csv-ingestion/src/repo.ts` — `ImportBatchRepo` (THE security boundary, INV-W1–W7):
   - `claimBatch()`, `reapStaleBatches()`, `updateHeartbeat()`, `updateProgress()`, `completeBatch()`, `failBatch()`, `insertRows()`
7. `workers/csv-ingestion/src/claim.ts` — reaper + claim orchestration
8. `workers/csv-ingestion/src/ingest.ts` — stream → parse → normalize → validate → insert pipeline
9. `workers/csv-ingestion/src/normalize.ts` — row normalization using shared `lib/csv/`
10. `workers/csv-ingestion/src/validate.ts` — validates against `importPlayerV1Schema` (reused from `services/player-import/schemas.ts`)
11. `workers/csv-ingestion/src/storage.ts` — signed URL generation + streaming fetch
12. `workers/csv-ingestion/src/logger.ts` — structured JSON logger (no `console.*`)
13. `workers/csv-ingestion/src/health.ts` — HTTP health + readiness endpoints

**Key Design Decisions**:
- Uses `pg` (node-postgres) directly for DB — NOT Supabase JS client for queries
- Supabase JS client used ONLY for Storage signed URL generation
- All DB access through `ImportBatchRepo` — no raw queries elsewhere
- 500-row chunk inserts with `ON CONFLICT (batch_id, row_number) DO NOTHING`
- Heartbeat every chunk or every 30s (whichever comes first)
- Row cap: stream stops at row 10,001 → batch transitions to `failed` with `last_error_code = 'BATCH_ROW_LIMIT'`. **Error signaling is state-based, not HTTP**: the worker sets `import_batch.last_error_code` and `import_batch.status = 'failed'`; the UI discovers the error via `GET /batches/:id` polling (standard 200 response with `status='failed'` and `last_error_code='BATCH_ROW_LIMIT'` in the batch DTO). No HTTP 413 is involved — this is not an upload-time rejection.

**Config defaults** (`config.ts`):
- `POLL_INTERVAL_MS = 5_000` — how often worker polls for `uploaded` batches
- `REAPER_HEARTBEAT_THRESHOLD_MS = 300_000` — 5 minutes; stale `parsing` batches older than this are reaped
- `STORAGE_SIGNED_URL_EXPIRY_SECONDS = 600` — 10 minutes; must exceed max parse time for 10k-row file
- `MAX_ATTEMPTS = 3` — reaper transitions to `failed` when `attempt_count >= MAX_ATTEMPTS`
- `CHUNK_SIZE = 500` — rows per bulk insert
- `STATEMENT_TIMEOUT_MS = 60_000` — per-connection statement timeout

**INV-W7 reaper clarification**: INV-W7 ("worker may only set `parsing`, `staging`, or `failed`") applies to normal processing transitions. The reaper module additionally sets `uploaded` (reset for retry) — this is a documented exception per ADR-037 INV-W2. The `ImportBatchRepo.reapStaleBatches()` method is the sole code path that sets `uploaded`, and it is constrained by INV-W2's WHERE clause.

**Acceptance Criteria**:
- [ ] Worker compiles (`cd workers/csv-ingestion && npx tsc --noEmit`)
- [ ] CI denylist grep returns empty (no forbidden table references)
- [ ] Health endpoint responds on configured port

### WS4: Upload Route Handler

**Purpose**: `POST /api/v1/player-import/batches/[id]/upload` — multipart/form-data file upload.

**Deliverables**:
1. `app/api/v1/player-import/batches/[id]/upload/route.ts`:
   - Follows existing execute/route.ts pattern (`withServerAction`, `createRequestContext`)
   - Next.js native `request.formData()` for multipart parsing
   - Validates batch `status = 'created'` (409 `IMPORT_BATCH_NOT_CREATED` otherwise)
   - File size limit: 10MB
   - Derives `casino_id` from auth context (ADR-024)
   - **Storage client**: Use `createServiceClient()` (`service_role`) for Supabase Storage upload. The authenticated user's Supabase client (via `withServerAction`) is used for auth context check and batch status validation. The `service_role` client is needed because no Storage bucket policies are created for authenticated users — all Storage access is server-side only (SEC note).
   - Storage path: `imports/{casino_id}/{batch_id}/{upload_id}.csv` (generated, never raw filename)
   - Stores `original_file_name` on `import_batch` for UI display
   - Transitions batch: `created → uploaded`
   - Requires `Idempotency-Key` header
   - Role gate: `admin`, `pit_boss`

**Acceptance Criteria**:
- [ ] Route follows `withServerAction` + `ServiceHttpResult<T>` pattern
- [ ] `casino_id` never accepted from request body (ADR-024 INV-8)
- [ ] Storage path uses generated UUID, not user-provided filename
- [ ] `npm run lint` passes

### WS5: Service Layer Updates

**Purpose**: Extend existing `services/player-import/` for worker lifecycle fields.

**Deliverables** (all EXTEND, not recreate):
1. `dtos.ts` — add `storage_path`, `original_file_name`, `claimed_by`, `claimed_at`, `heartbeat_at`, `attempt_count`, `last_error_at`, `last_error_code` to `ImportBatchDTO` Pick list; add `ImportIngestionReportV1` interface
2. `schemas.ts`:
   - Add `uploadFileParamSchema` (Zod for upload route params)
   - Update `batchListQuerySchema` status enum to include `created`, `uploaded`, `parsing`
   - Update `createBatchSchema` to add optional `initial_status: z.enum(['staging', 'created']).optional()` — server flow passes `'created'`, old flow omits (defaults to `'staging'` in RPC)
   - **INV-UI-1 (governance invariant):** The server wizard path (WS6) MUST always pass `initial_status: 'created'` when calling `createBatch()`. Only the legacy Lane 1 `useStagingUpload` flow (deprecated) uses the 4-param default-to-staging overload. If `initial_status` is omitted in the new wizard flow, batches default to `staging` and the upload endpoint will 409 — this is the exact P0-1 failure mode. Enforcement: the `use-import-wizard.ts` step machine hardcodes `initial_status: 'created'` in its batch-creation step; unit test asserts this invariant.
3. `crud.ts`:
   - Add `uploadFile()` method (validates batch status, uploads to Storage via `service_role` client, updates batch)
   - Update `createBatch()` to pass `p_initial_status` to the new 5-param RPC overload when `initial_status` is provided in input. When omitted, calls the existing 4-param overload (backward compat).
4. `http.ts` — add `uploadFile()` HTTP fetcher (multipart/form-data); update `createBatch()` to include optional `initial_status` in request body
5. `mappers.ts` — extend `ImportBatchSelectedRow` type and `toImportBatchDTO` mapper; add `toImportIngestionReportV1` mapper
6. `index.ts` — add `uploadFile` to `PlayerImportServiceInterface` and factory

**Acceptance Criteria**:
- [ ] DTOs follow Pattern B (Pick from Database types)
- [ ] `npm run type-check` passes
- [ ] Functional factory pattern maintained
- [ ] `createBatch({ ..., initial_status: 'created' })` calls 5-param RPC overload and creates batch with `status = 'created'`
- [ ] `createBatch({ ... })` (no initial_status) creates batch with `status = 'staging'` (backward compat)

### WS6: UI Updates — Import Wizard

**Purpose**: Update wizard step machine for server-authoritative flow.

**Deliverables**:
1. `hooks/player-import/use-import-wizard.ts` — MODIFY: step machine changes:
   - Old: `file-selection → column-mapping → preview → staging-upload → execute → report`
   - New: `file-selection → column-mapping → preview → file-upload → worker-processing → execute → report`
2. `hooks/player-import/use-file-upload.ts` — CREATE: multipart upload with `useTransition`
3. `hooks/player-import/use-batch-polling.ts` — CREATE: TanStack Query polling with adaptive backoff (4s → 12s after 60s)
4. `hooks/player-import/use-staging-upload.ts` — DEPRECATE (add `@deprecated` JSDoc, keep for rollback)
5. `components/player-import/step-file-upload.tsx` — CREATE: file upload step with progress
6. `components/player-import/step-worker-processing.tsx` — CREATE: worker progress display (status, row counts, errors). When `status='failed'`, reads `last_error_code` from batch DTO and maps to user-facing message (e.g., `BATCH_ROW_LIMIT` → "CSV exceeds 10,000-row limit"). Error is discovered via polling, not via HTTP error response.
7. `components/player-import/ingestion-report-card.tsx` — CREATE: ingestion report with overwrite warning
8. `components/player-import/import-wizard.tsx` — MODIFY: use new steps, import new hooks

**Critical Invariant (INV-UI-1):**
The server wizard batch-creation step MUST pass `initial_status: 'created'` to `createBatch()`. This is hardcoded in the `use-import-wizard.ts` step machine — not a UI toggle, not a user choice. Omitting it causes the P0-1 failure mode (batch defaults to `staging`, upload endpoint 409s every request).

**Polling Contract**:
- 4s while `status = 'parsing'`
- Backoff to 12s after 60 seconds of polling
- Stop polling on `status = 'staging'` or `status = 'failed'`

**Acceptance Criteria**:
- [ ] No `useEffect` sync patterns (React 19 compliance)
- [ ] All async operations use `useTransition` (no manual loading states)
- [ ] Polling stops on terminal states
- [ ] Ingestion report displayed before execute with overwrite warning
- [ ] `npm run build` passes

### WS7: Unit Tests — Worker & Shared Modules

**Purpose**: Verify correctness of shared normalization and worker logic.

**Deliverables**:
1. `lib/csv/__tests__/header-normalization.test.ts` — trim, BOM, blank, duplicate, newline, mixed scenarios
2. `workers/csv-ingestion/__tests__/repo.test.ts` — INV-W1–W7 SQL pattern verification (mocked pg)
3. `workers/csv-ingestion/__tests__/normalize.test.ts` — column mapping, missing fields, error cases
4. `workers/csv-ingestion/__tests__/validate.test.ts` — ImportPlayerV1 schema validation
5. `workers/csv-ingestion/__tests__/ingest.test.ts` — chunk insertion, row cap, heartbeat, report (mocked repo)

**Acceptance Criteria**:
- [ ] All unit tests pass
- [ ] Repo tests verify SQL invariants (WHERE clauses, ON CONFLICT, status restrictions)

### WS8: Integration Tests

**Purpose**: Verify end-to-end worker behavior with real database.

**Deliverables**:
1. `workers/csv-ingestion/__tests__/ingest.int.test.ts` — 100-row and 10k-row CSV ingestion
2. `workers/csv-ingestion/__tests__/cross-casino.int.test.ts` — 2-casino isolation (INV-W3, INV-W5)
3. `workers/csv-ingestion/__tests__/crash-recovery.int.test.ts` — mid-parse abort → reaper → re-ingest → semantically equivalent output
4. `workers/csv-ingestion/__tests__/concurrent-claim.int.test.ts` — two concurrent claims, one wins (SKIP LOCKED)
5. `workers/csv-ingestion/__tests__/denylist.test.ts` — CI denylist grep (no forbidden table references)
6. `services/player-import/__tests__/upload-route.int.test.ts` — upload to created→200, staging→409, size limit
7. `services/player-import/__tests__/execute-guard.int.test.ts` — execute on failed→rejection

**Test Fixtures** (`workers/csv-ingestion/__tests__/fixtures/`):
- `small-valid.csv` (10 rows), `mixed-valid-invalid.csv` (20 rows), `malformed.csv`
- Generated: `large-10k.csv`, `oversized-10001.csv`

**Acceptance Criteria**:
- [ ] All integration tests pass
- [ ] Cross-casino test verifies zero row leakage
- [ ] Crash recovery produces semantically equivalent output (idempotent)
- [ ] Denylist grep returns empty

### WS9: E2E Tests — Server CSV Import

**Purpose**: Verify full user journey through the updated import wizard.

**Deliverables**:
1. `e2e/workflows/csv-server-import.spec.ts`:
   - Happy path: upload → worker processes → staging → execute → report
   - Error path: oversized CSV (>10k rows) → failed → UI shows error
   - Progress display: status transitions + row counts during parsing
2. `e2e/fixtures/sample-csvs/server-import-10-rows.csv` — small valid CSV
3. `e2e/fixtures/sample-csvs/server-import-mixed.csv` — mixed valid/invalid rows

**Prerequisites**: Worker process must be running in test environment.

**Acceptance Criteria**:
- [ ] Happy path E2E passes (180s timeout for worker processing)
- [ ] Failed batch disables execute button
- [ ] Playwright auto-waiting (no arbitrary timeouts)

---

## Definition of Done

### Functionality
- [ ] Upload endpoint accepts CSV, stores in Storage, transitions batch to `uploaded`
- [ ] Worker claims `uploaded` batches, streams, parses, normalizes, stages rows
- [ ] Worker output semantically equivalent to client-side staging (canonical JSON comparison)
- [ ] Batch status machine: `created → uploaded → parsing → staging` (with `failed` terminal)
- [ ] Failed batches reach terminal `failed` status (row cap, parse error, max attempts)
- [ ] Upload endpoint rejects non-`created` batches with 409
- [ ] Server flow creates batches with `status = 'created'` via 5-param RPC overload (DA P0-1 fix)
- [ ] Old client flow creates batches with `status = 'staging'` via existing 4-param RPC (backward compat)
- [ ] `rpc_import_execute` rejects `failed` batches with `IMPORT_BATCH_NOT_STAGING` exception (DA P0-2 fix)
- [ ] `rpc_import_execute` still returns silently for `completed` batches (idempotent)
- [ ] Import wizard updated with file-upload and worker-processing steps
- [ ] Client polls and displays batch progress during worker processing

### Data & Integrity
- [ ] `import_batch` extended with all worker lifecycle columns
- [ ] `UNIQUE(batch_id, row_number)` constraint confirmed on `import_row` (migration asserts via `pg_constraint` lookup — fails loudly if missing)
- [ ] `import_batch_status` enum extended with `created`, `uploaded`, `parsing`
- [ ] Crash recovery: kill mid-parse → restart → semantically equivalent staged rows
- [ ] Reaper resets stale batches; fails exhausted batches

### Security & Access
- [ ] Upload endpoint derives `casino_id` from auth context (ADR-024)
- [ ] Upload endpoint requires `admin` or `pit_boss` role
- [ ] `storage_path` uses generated UUID, never raw filename
- [ ] Worker `casino_id` from claimed batch row only (INV-W5)
- [ ] All worker DB access through `ImportBatchRepo` (INV-W4)
- [ ] Worker writes only to `import_batch` + `import_row` (INV-W4)
- [ ] Worker only sets status to `parsing`, `staging`, or `failed` (INV-W7)
- [ ] 2-casino integration test verifies zero cross-casino writes

### Testing
- [ ] Unit tests: header normalization, repo invariants, row validation, claim/reaper
- [ ] Integration tests: 10k-row ingestion, cross-casino, crash recovery, concurrent claim
- [ ] Integration test: upload to non-`created` batch → 409
- [ ] Integration test: execute on `failed` batch → `IMPORT_BATCH_NOT_STAGING` exception (DA P0-2)
- [ ] Integration test: execute on `completed` batch → silent return (idempotent, unchanged)
- [ ] Integration test: create batch with `initial_status='created'` → status is `created` (DA P0-1)
- [ ] Integration test: create batch without `initial_status` → status is `staging` (backward compat)
- [ ] E2E test: upload → worker → staging → execute → report (happy path)
- [ ] CI denylist grep: worker references no forbidden tables
- [ ] Migration guard: UNIQUE constraint confirmed

### Operational Readiness
- [ ] Worker health endpoint + restart policy configured
- [ ] Structured JSON logs with batch_id, status, row counts, duration
- [ ] Rollback path defined: stop worker → batches stay `uploaded` → old client flow available

### Documentation
- [ ] Upload endpoint documented in API_SURFACE_MVP.md
- [ ] SRM updated with worker process and Storage dependency
- [ ] Migration phases (A→D) documented
- [ ] Known limitations documented (no file retention, no raw_row redaction)

---

## Adversarial Review (Devil's Advocate)

**Verdict:** Ship w/ gates
**Reviewer:** devils-advocate skill
**Date:** 2026-02-24

**Version History:**
- v1.0.0 — Initial EXEC-SPEC (DA review identified P0 findings)
- v1.1.0 — DA P0 fixes + P1 clarifications applied
- v1.2.0 — User review P1 patches (INV-UI-1, INV-W2 full clause, UNIQUE migration guard, error signaling clarification)

### P0 Fixes Applied (v1.1.0)

| ID | Finding | Fix |
|----|---------|-----|
| P0-1 | No workstream creates batches with `created` status — `rpc_import_create_batch` defaults to `staging`, upload endpoint will 409 every request | Added WS1 migration `20260225145409` to add `p_initial_status` parameter overload to `rpc_import_create_batch`. Updated WS5 to pass `initial_status` through crud/schemas. |
| P0-2 | `rpc_import_execute` silently returns for `failed` batches instead of rejecting — contradicts PRD-039 §5.1 DoD | Added WS1 migration `20260225145410` to patch execute RPC: `IN ('completed', 'failed')` → `= 'completed'`. Failed batches now raise `IMPORT_BATCH_NOT_STAGING`. |

### P1 Clarifications Applied (v1.1.0)

| ID | Finding | Clarification |
|----|---------|---------------|
| P1-1 | Upload route Supabase client not specified | WS4: Use `createServiceClient()` (service_role) for Storage upload; user client for auth only |
| P1-2 | INV-W7 vs reaper inconsistency | WS3 + Security Posture table: INV-W7 scoped to normal processing; reaper `uploaded` reset is documented INV-W2 exception |
| P1-3 | `file_name` vs `original_file_name` redundancy | WS1 notes: `file_name` = batch-creation label; `original_file_name` = actual uploaded file name; they may differ |
| P1-4 | Worker config defaults unspecified | WS3: Added `POLL_INTERVAL_MS`, `REAPER_HEARTBEAT_THRESHOLD_MS`, `STORAGE_SIGNED_URL_EXPIRY_SECONDS`, `MAX_ATTEMPTS`, `CHUNK_SIZE`, `STATEMENT_TIMEOUT_MS` |
| P1-5 | Storage bucket policies unspecified | WS1: Bucket is private, no `storage.objects` policies created — all access via service_role |

### P1 Fixes Applied (v1.2.0 — User Review)

| ID | Finding | Fix |
|----|---------|-----|
| P1-R1 | Status machine governance trap — no invariant ensures server wizard passes `initial_status='created'` | Added INV-UI-1 invariant in WS5 + WS6: server wizard MUST hardcode `initial_status: 'created'`; unit test asserts. |
| P1-R2 | INV-W2 incomplete in Security Posture table — missing `attempt_count < max_attempts` clause | Updated INV-W2 to include full condition with `attempt_count >= max_attempts → failed` transition. |
| P1-R3 | UNIQUE constraint is "trust me bro" — relied on but not migration-guarded | Added `pg_constraint` assertion in migration `20260225145407` — fails loudly if constraint missing. |
| P1-R4 | `IMPORT_BATCH_ROW_LIMIT` implied HTTP 413 but it's state-based | Clarified: error signaling via `last_error_code` in batch DTO, discovered by polling; not an HTTP error. Added UI mapping note in WS6. |

### Accepted P2 Risks (not blocking)

- No DB constraint enforcing `import_row.casino_id = import_batch.casino_id` (app-enforced via INV-W3/W5 + 2-casino integration test)
- No DB-level status machine constraint (app-enforced via RPC + worker repo)
- Worker file decomposition (12 files) may be over-modular for simple logic — implementation can consolidate if appropriate
